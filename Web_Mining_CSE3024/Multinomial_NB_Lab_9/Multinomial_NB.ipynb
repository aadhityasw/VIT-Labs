{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Multinomial_NB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2875MBR38jP"
      },
      "source": [
        "# Multinomial Naive Bayes\n",
        "\n",
        "In this notebook, we perform a `Multinomial Naive Bayes` Classification on the `US Airline Tweet` Dataset, using sentiment analysis.\n",
        "\n",
        "**Reference**\n",
        "* [Multinomial NB](https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlXS1ftH38jR"
      },
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BgGmQfd38jS"
      },
      "source": [
        "## 1. Import the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "wTI5Yn9C38jT",
        "outputId": "ba4c65b8-d6c3-4c83-e75a-c4b1268922ac"
      },
      "source": [
        "df = pd.read_csv(\"airline_tweet_processed.csv\")\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>said</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>plus added commercial experience tacky</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>today must mean need take another trip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>really big bad thing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   airline_sentiment  ...                                               text\n",
              "0                  1  ...                                               said\n",
              "1                  2  ...             plus added commercial experience tacky\n",
              "2                  1  ...             today must mean need take another trip\n",
              "3                  0  ...  really aggressive blast obnoxious entertainmen...\n",
              "4                  0  ...                               really big bad thing\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te6stq0Q38jU"
      },
      "source": [
        "**Fields**\n",
        "\n",
        "* `airline_sentiment` : Sentiment\n",
        "                           `0` - `Negative`\n",
        "                           `1` - `Neutral`\n",
        "                           `2` - `Positive`\n",
        "* `airline` : Name of the Airline\n",
        "* `text` : The words in the pre-processed tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r5irff_38jU"
      },
      "source": [
        "## 2. Perform test train split\n",
        "\n",
        "We allocate about 20% of the data for testing and the remaining will be used to train the model.\n",
        "<br>\n",
        "The input variable is the processed `text`, and the output variable is `airline_sentiment`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaV6CC3F38jV"
      },
      "source": [
        "output_classes = ['Negative', 'Neutral', 'Positive']\n",
        "num_classes = len(df['airline_sentiment'].unique())\n",
        "num_tweets = len(df)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N0_rF2w38jW"
      },
      "source": [
        "train_percentage = 0.8\n",
        "num_train = int(train_percentage * num_tweets)\n",
        "num_test = num_tweets - num_train"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB18fSVs8kfU",
        "outputId": "7cd85dfc-b245-405b-b046-b794419e3b3b"
      },
      "source": [
        "print(\"Train set size : \", num_train)\n",
        "print(\"Test set size : \", num_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set size :  11712\n",
            "Test set size :  2928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acCg1HU038jW"
      },
      "source": [
        "# Shuffle your dataset \n",
        "\n",
        "shuffle_df = df.sample(frac=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxVwbB-j38jX"
      },
      "source": [
        "train_df = shuffle_df[:num_train]\n",
        "test_df = shuffle_df[num_train:]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbt0DWW68PdN"
      },
      "source": [
        "train_df.to_csv(\"train.csv\", index=False)\n",
        "test_df.to_csv(\"test.csv\", index=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTNHuNGG38jX",
        "outputId": "2c4d4cfe-5f62-4ce4-b54f-fae9e4a81827"
      },
      "source": [
        "print(f\"The training set has {num_train} sets of values.\")\n",
        "print(f\"The testing set has {num_test} sets of values.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training set has 11712 sets of values.\n",
            "The testing set has 2928 sets of values.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQSfE4_C38jY"
      },
      "source": [
        "## 3. Class Distribution\n",
        "\n",
        "![Class Probability image](./img/class_probability.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTduqp2i38jZ"
      },
      "source": [
        "# Find the number of tweets of each class\n",
        "\n",
        "probability_class = np.array([train_df[train_df['airline_sentiment'] == i]['airline_sentiment'].count() for i in range(num_classes)])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcKLC2Q138jZ"
      },
      "source": [
        "# Divide the values by the total number of tweets to get the probability of each class\n",
        "\n",
        "probability_class = probability_class / num_train"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O1RK3Ri38ja"
      },
      "source": [
        "# Convert this into a dictionary for better access\n",
        "\n",
        "probability_class = {\n",
        "    i : probability_class[i] for i in range(num_classes)\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBoshNWF38ja",
        "outputId": "1b5cdceb-7b17-404c-9442-d2ba28cef826"
      },
      "source": [
        "# Display the Class Probabilities\n",
        "\n",
        "print(\"Class probabilities : \\n\")\n",
        "for i in range(num_classes) :\n",
        "    print(output_classes[i], \" : \", probability_class[i])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class probabilities : \n",
            "\n",
            "Negative  :  0.6239754098360656\n",
            "Neutral  :  0.21174863387978143\n",
            "Positive  :  0.164275956284153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGyeGXV638jb"
      },
      "source": [
        "## 4. Probability Distribution over Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ecK5dBI38jb"
      },
      "source": [
        "### 4.1 Prepare the Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCxHGA1i38jc"
      },
      "source": [
        "# Initialize a set to store all the words\n",
        "\n",
        "vocabulary = set()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuusPLou38jc"
      },
      "source": [
        "# Function to extract the vocabulary\n",
        "\n",
        "def extractVocabulary(tweet) :\n",
        "    for word in str(tweet).split(\" \") :\n",
        "        vocabulary.add(word)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRGagWO338jd"
      },
      "source": [
        "# Find all the unique words\n",
        "\n",
        "_ = train_df['text'].apply(extractVocabulary)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qsgX96l38jd"
      },
      "source": [
        "# Convert the vocabulary into a list\n",
        "\n",
        "vocabulary = list(vocabulary)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7sLzsu538je"
      },
      "source": [
        "vocabulary_count = len(vocabulary)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1caul2mM38jf"
      },
      "source": [
        "# Save this vocabulary\n",
        "\n",
        "vocabulary_df = pd.DataFrame(columns=['index', 'word'])\n",
        "vocabulary_df['word'] = vocabulary\n",
        "vocabulary_df['index'] = [i for i in range(len(vocabulary))]\n",
        "vocabulary_df.head()\n",
        "vocabulary_df.to_csv('vocabulary_mapping.csv', index=False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_IEbnH38jf"
      },
      "source": [
        "### 4.2 Form the Word Distribution Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIPGQbUt38jf"
      },
      "source": [
        "word_distribution_df = pd.DataFrame(columns=['tweet_idx', 'word_idx', 'count', 'class_idx'])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxp9J9mJ38jg"
      },
      "source": [
        "i = 0\n",
        "def extractWordDistribution(row) :\n",
        "    global word_distribution_df, i\n",
        "    \n",
        "    tweet = row['text']\n",
        "    temp_words = str(tweet).split(\" \")\n",
        "    temp_word_count = collections.Counter(temp_words)\n",
        "    temp_word_count_arr = []\n",
        "    temp_word_idx_arr = []\n",
        "    for temp_word, temp_count in temp_word_count.items() :\n",
        "        temp_word_idx_arr.append(int(vocabulary_df[vocabulary_df['word'] == temp_word]['index']))\n",
        "        temp_word_count_arr.append(temp_count)\n",
        "    \n",
        "    # Concatenate the rows into the dataset\n",
        "    temp_df = pd.DataFrame({\n",
        "        'tweet_idx' : [i]*len(temp_word_count_arr),\n",
        "        'word_idx' : temp_word_idx_arr,\n",
        "        'count' : temp_word_count_arr,\n",
        "        'class_idx' : [row['airline_sentiment']]*len(temp_word_count_arr)\n",
        "    })\n",
        "    word_distribution_df = pd.concat([\n",
        "        word_distribution_df,\n",
        "        temp_df\n",
        "    ],ignore_index=True)\n",
        "    \n",
        "    i += 1\n",
        "    if i % 1000 == 0 :\n",
        "        print(i)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iZ7l3zf38jg",
        "outputId": "bae1f018-8afe-4fcc-f592-dba63406f3aa"
      },
      "source": [
        "_ = train_df.apply(extractWordDistribution, axis=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzDScirY38jh"
      },
      "source": [
        "# Save this distribution\n",
        "\n",
        "word_distribution_df.to_csv(\"word_distribution.csv\", index=False)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBVZIfKS38ji"
      },
      "source": [
        "### 4.3 Probability of each word per class\n",
        "\n",
        "For class `j` and word `i`, the average is given by:\n",
        "<br>\n",
        "![Word Class Probability Formula](./img/word_class_probability.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOeZjC8t38jj"
      },
      "source": [
        "# Smoothing\n",
        "\n",
        "alpha = 0.001"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKMxGe9y38jj"
      },
      "source": [
        "#Calculate probability of each word based on class\n",
        "\n",
        "pb_ij = word_distribution_df.groupby(['class_idx','word_idx'])\n",
        "pb_j = word_distribution_df.groupby(['class_idx'])\n",
        "Pr =  (pb_ij['count'].sum() + alpha) / (pb_j['count'].sum() + vocabulary_count)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9eomiVQ38jj"
      },
      "source": [
        "#Unstack series\n",
        "\n",
        "Pr = Pr.unstack()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "GySmDG9Z38jk",
        "outputId": "e484367b-c090-4b30-f947-f963fec17b5a"
      },
      "source": [
        "Pr"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>word_idx</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>9076</th>\n",
              "      <th>9077</th>\n",
              "      <th>9078</th>\n",
              "      <th>9079</th>\n",
              "      <th>9080</th>\n",
              "      <th>9081</th>\n",
              "      <th>9082</th>\n",
              "      <th>9083</th>\n",
              "      <th>9084</th>\n",
              "      <th>9085</th>\n",
              "      <th>9086</th>\n",
              "      <th>9087</th>\n",
              "      <th>9088</th>\n",
              "      <th>9089</th>\n",
              "      <th>9090</th>\n",
              "      <th>9091</th>\n",
              "      <th>9092</th>\n",
              "      <th>9093</th>\n",
              "      <th>9094</th>\n",
              "      <th>9095</th>\n",
              "      <th>9096</th>\n",
              "      <th>9097</th>\n",
              "      <th>9098</th>\n",
              "      <th>9099</th>\n",
              "      <th>9100</th>\n",
              "      <th>9101</th>\n",
              "      <th>9102</th>\n",
              "      <th>9103</th>\n",
              "      <th>9104</th>\n",
              "      <th>9105</th>\n",
              "      <th>9106</th>\n",
              "      <th>9107</th>\n",
              "      <th>9108</th>\n",
              "      <th>9109</th>\n",
              "      <th>9110</th>\n",
              "      <th>9111</th>\n",
              "      <th>9112</th>\n",
              "      <th>9113</th>\n",
              "      <th>9114</th>\n",
              "      <th>9115</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class_idx</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000929</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005047</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000477</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000565</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000314</td>\n",
              "      <td>0.000515</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.000013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000228</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.002777</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000685</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000495</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000611</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.002923</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000305</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.000305</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 9116 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "word_idx       0         1         2     ...      9113      9114      9115\n",
              "class_idx                                ...                              \n",
              "0          0.000038  0.000025       NaN  ...       NaN  0.000113  0.000013\n",
              "1          0.000228       NaN  0.000038  ...       NaN  0.000114       NaN\n",
              "2          0.000044       NaN  0.000044  ...  0.000044       NaN       NaN\n",
              "\n",
              "[3 rows x 9116 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJgT-p0P38jl"
      },
      "source": [
        "#Replace NaN or columns with 0 as word count with a/(count+|V|+1)\n",
        "\n",
        "for c in range(1,num_classes):\n",
        "    Pr.loc[c,:] = Pr.loc[c,:].fillna(alpha/(pb_j['count'].sum()[c] + vocabulary_count))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lspI_18F38jm"
      },
      "source": [
        "#Convert to dictionary for better access\n",
        "\n",
        "Pr_dict = Pr.to_dict()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugGZHtJT38jm"
      },
      "source": [
        "## 5. Multinomial Naive Bayes\n",
        "\n",
        "![MNB image](./img/mnb.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUkFW6a-38jm"
      },
      "source": [
        "def MultinomialNaiveBayes(data) :\n",
        "    '''\n",
        "    Multinomial Naive Bayes classifier\n",
        "    :param data [Pandas Dataframe]: Dataframe of data\n",
        "    :return predict [list]: Predicted class ID\n",
        "    '''\n",
        "    \n",
        "    #Using dictionaries for greater speed\n",
        "    df_dict = data.to_dict()\n",
        "    new_dict = {}\n",
        "    predictions = []\n",
        "    \n",
        "    # new_dict = {docIdx : {wordIdx: count},....}\n",
        "    for idx in range(len(df_dict['tweet_idx'])):\n",
        "        tweetIdx = df_dict['tweet_idx'][idx]\n",
        "        wordIdx = df_dict['word_idx'][idx]\n",
        "        count = df_dict['count'][idx]\n",
        "        try: \n",
        "            new_dict[tweetIdx][wordIdx] = count\n",
        "        except:\n",
        "            new_dict[df_dict['tweet_idx'][idx]] = {}\n",
        "            new_dict[tweetIdx][wordIdx] = count\n",
        "        \n",
        "    # Calculating the scores for each tweet\n",
        "    for tweetIdx in new_dict.keys():\n",
        "        score_dict = {}\n",
        "        # Creating a probability row for each class\n",
        "        for classIdx in range(1,num_classes):\n",
        "            score_dict[classIdx] = 1\n",
        "            # For each word:\n",
        "            for wordIdx in new_dict[tweetIdx]:\n",
        "                try:\n",
        "                    # Use frequency smoothing\n",
        "                    # log(1+f)*log(Pr(i|j))\n",
        "                    probability=Pr_dict[wordIdx][classIdx]         \n",
        "                    power = np.log(1+ new_dict[tweetIdx][wordIdx])     \n",
        "                    score_dict[classIdx]+=power*np.log(probability)\n",
        "                except:\n",
        "                    # Missing V will have log(1+0)*log(a/num_classes)=0 \n",
        "                    score_dict[classIdx] += 0\n",
        "            # Multiply final with probability of the class\n",
        "            score_dict[classIdx] +=  np.log(probability_class[classIdx])\n",
        "    \n",
        "        #Get class with max probabilty for the given docIdx \n",
        "        max_score = max(score_dict, key=score_dict.get)\n",
        "        predictions.append(max_score)\n",
        "        \n",
        "    return predictions"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-7eF1IZ38jn"
      },
      "source": [
        "## 6. Make Predictions of the train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2hFeAMt38jn"
      },
      "source": [
        "Y_train_pred = MultinomialNaiveBayes(word_distribution_df)\n",
        "Y_train = train_df['airline_sentiment'].tolist()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBsKRv5_38jo"
      },
      "source": [
        "# Save the train predictions\n",
        "\n",
        "np.save('y_train_predictions.npy', np.array(Y_train_pred))"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dotDX2o38jo"
      },
      "source": [
        "# Load the saved predictions\n",
        "\n",
        "Y_train_pred = list(np.load('y_train_predictions.npy'))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6KBQKsf38jp"
      },
      "source": [
        "# Calculate the Training Error\n",
        "\n",
        "error = 0\n",
        "\n",
        "for (i, j) in zip(Y_train_pred, Y_train) :\n",
        "    if i != j :\n",
        "        error += 1"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IYzn4yO38jp",
        "outputId": "6b639421-0d1c-44fb-ee62-083ff71aa637"
      },
      "source": [
        "train_error_rate = error * 100 / num_train\n",
        "print(\"Training Error : \", train_error_rate, \"%\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Error :  64.79678961748634 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2-JWpVG38jq",
        "outputId": "0a2b8680-e833-4d44-ce7d-d1052bf58676"
      },
      "source": [
        "train_accuracy = 100 - train_error_rate\n",
        "print(\"Training Accuracy : \", train_accuracy, \"%\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy :  35.20321038251366 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmk51wi838jq"
      },
      "source": [
        "## 7. Test the model on Unseen data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIEH0ybs38jq"
      },
      "source": [
        "# Form the Vocabulary from test set\n",
        "\n",
        "# Initialize a set to store all the words\n",
        "test_vocabulary = set()\n",
        "\n",
        "# Function to extract the vocabulary\n",
        "def extractTestVocabulary(tweet) :\n",
        "    for word in str(tweet).split(\" \") :\n",
        "        test_vocabulary.add(word)\n",
        "\n",
        "# Find all the unique words\n",
        "_ = test_df['text'].apply(extractTestVocabulary)\n",
        "\n",
        "# Convert the vocabulary into a list\n",
        "test_vocabulary = list(test_vocabulary)\n",
        "\n",
        "# Find the number of words in the vocabulary => |V_test|\n",
        "test_vocabulary_count = len(test_vocabulary)\n",
        "\n",
        "# Convert it into a dataframe\n",
        "test_vocabulary_df = pd.DataFrame(columns=['index', 'word'])\n",
        "test_vocabulary_df['word'] = test_vocabulary\n",
        "test_vocabulary_df['index'] = [i for i in range(test_vocabulary_count)]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6nZkg2Z38jr"
      },
      "source": [
        "# Word Distribution\n",
        "\n",
        "# Initialize a dataframe to store these details\n",
        "test_word_distribution_df = pd.DataFrame(columns=['tweet_idx', 'word_idx', 'count', 'class_idx'])\n",
        "\n",
        "i = 0\n",
        "def extractTestWordDistribution(row) :\n",
        "    global test_word_distribution_df, i\n",
        "    # Extract the count of words\n",
        "    tweet = row['text']\n",
        "    temp_words = str(tweet).split(\" \")\n",
        "    temp_word_count = collections.Counter(temp_words)\n",
        "    temp_word_count_arr = []\n",
        "    temp_word_idx_arr = []\n",
        "    for temp_word, temp_count in temp_word_count.items() :\n",
        "        temp_word_idx_arr.append(int(test_vocabulary_df[test_vocabulary_df['word'] == temp_word]['index']))\n",
        "        temp_word_count_arr.append(temp_count)\n",
        "    # Concatenate the rows into the dataset\n",
        "    temp_df = pd.DataFrame({\n",
        "        'tweet_idx' : [i]*len(temp_word_count_arr),\n",
        "        'word_idx' : temp_word_idx_arr,\n",
        "        'count' : temp_word_count_arr,\n",
        "        'class_idx' : [row['airline_sentiment']]*len(temp_word_count_arr)\n",
        "    })\n",
        "    test_word_distribution_df = pd.concat([\n",
        "        test_vocabulary_df,\n",
        "        temp_df\n",
        "    ],ignore_index=True)\n",
        "    # Increment the index\n",
        "    i += 1\n",
        "\n",
        "_ = test_df.apply(extractTestWordDistribution, axis=1)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQohwFqU38jr",
        "outputId": "3f40ded4-f907-467a-ab7d-7c5f0e5eff87"
      },
      "source": [
        "# Make the predictions and use to to calculate the error rate\n",
        "Y_test_pred = MultinomialNaiveBayes(test_word_distribution_df)\n",
        "Y_test = test_df['airline_sentiment'].tolist()\n",
        "\n",
        "# Calculate the Training Error\n",
        "error = 0\n",
        "for (i, j) in zip(Y_test_pred, Y_test_pred) :\n",
        "    if i != j :\n",
        "        error += 1\n",
        "\n",
        "test_error_rate = error * 100 / num_test\n",
        "print(\"Testing Error : \", test_error_rate, \"%\")\n",
        "\n",
        "test_accuracy = 100 - test_error_rate\n",
        "print(\"Testing Accuracy : \", test_accuracy, \"%\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Error :  0.0 %\n",
            "Testing Accuracy :  100.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--OfgUMz38js"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}